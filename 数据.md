# 数据格式
  * 格式化
  * 半格式化
  * 文本
  * 富文本

# 数据技术关系
![数据技术关系图](https://github.com/star2478/server-tech-tree/blob/master/img/data-tech-relationship.png)


# 大数据相关技术
```
大数据组件生态圈
· HDFS =====> 解决存储问题
· MapReduce =====> 批处理框架
· Spark =====> 混合框架，能处理批处理和流计算
· Flink =====> 混合框架，能处理批处理和流计算
· Storm =====> 流计算框架
· Yarn =====> 资源协调者
· Zookeeper =====> 分布式应用程序协调服务
· Flume =====> 日志收集系统
· Hive =====> 基于Hadoop的数仓工具
· HBase =====> 分布式、面向列的开源数据库
· Sqoop =====> 数据传递工具
· Scala =====> 多范式编程语言、面向对象和函数式编程的特性
· Elasticsearch =====> 大数据分布式弹性搜索引擎
```
* 大数据技术方向（待改正）：数据收集处理、推荐、实时计算、离线计算（基于数仓）、数据分析挖掘

# 文件系统
* hdfs
 * namenode和datanode

# 大数据系统
* 另一方面，你看一下 Hadoop 几个主要产品的架构设计，就会发现它们都有相似性，都是一主多从的架构方案。HDFS，一个 NameNode，多个 DataNode；MapReduce 1，一个 JobTracker，多个 TaskTracker；Yarn，一个 ResourceManager，多个 NodeManager。
* 事实上，很多大数据产品都是这样的架构方案：Storm，一个 Nimbus，多个 Supervisor；Spark，一个 Master，多个 Slave。
* 大数据因为要对数据和计算任务进行统一管理，所以和互联网在线应用不同，需要一个全局管理者。而在线应用因为每个用户请求都是独立的，而且为了高性能和便于集群伸缩，会尽量避免有全局管理者。

## MapReduce
![MapReduce](https://github.com/star2478/server-tech-tree/blob/master/img/MapReduce.png)
* 关键能力：代码到数据所在地执行(边缘计算)、map输出的同一个key结果会shuffle到同一个reduce进程节点处理(对key做hash%reduce数，就可保证到同一个ID的reduce上，但当同一个key数据大且reduce数不够时，可能会压垮reduce)

## Spark
![Spark](https://github.com/star2478/server-tech-tree/blob/master/img/spark.png)
* 注：上图，application master可能和MapReduce作业(图中容器)不在同一个datanode
* spark：RDD 弹性数据集（Resilient Distributed Datasets），Spark 分布式计算的数据分片、任务调度都是以 RDD 为单位展开的，每个 RDD 分片都会分配到一个执行进程去处理。
* RDD 上定义的函数分两种，一种是转换（transformation）函数，这种函数的返回值还是 RDD；另一种是执行（action）函数，这种函数不再返回 RDD。
* RDD 定义了很多转换操作函数(很像rxjava)，比如有计算map(func)、过滤filter(func)、合并数据集union(otherDataset)、根据 Key 聚合reduceByKey(func, [numPartitions])、连接数据集join(otherDataset, [numPartitions])、分组groupByKey([numPartitions]) 等十几个函数。
* 相对RM的map和reduce，spark一个作业可以支持更多函数的计算，速度更快。spark也有关键的shuffle过程(一个数据集中的多个数据分片需要进行分区传输，写入到另一个数据集的不同分片中)，shuffle会把上一次计算结果按key进行数据分片，不同分片传到不同的下一个计算节点
* spark会将一个作业所有函数转化成dag(有向无环图，见上图)，rdd是一个个数据集，如图上，一个rdd包含了多个小块，每个小块代表一个数据分片，相当于rdd是逻辑上的数据集，里面的一个分片对应一个物理位置
* spark会将一个作业所有函数转化成dag(有向无环图，见上图)，rdd是一个个数据集，如图上，一个rdd包含了多个小块，每个小块代表一个数据分片，相当于rdd是逻辑上的数据集，里面的一个分片对应一个物理位置。
* 有些函数不会shuffle，比如上图map，c和d两个rdd数据量都一样。有些需要shuffle，比如上图groupby，rdd a转成一个数据量完全不同的rdd b，a结果要shuffle出去形成rdd b
* Spark 有三个主要特性：RDD 的编程模型更简单，DAG 切分的多阶段计算过程更快速，使用内存存储中间计算结果更高效。这三个特性使得 Spark 相对 Hadoop MapReduce 可以有更快的执行速度，以及更简单的编程实现。
* 首先，Spark 在 2012 年左右开始流行，那时内存的容量提升和成本降低已经比 MapReduce 出现的十年前强了一个数量级，Spark 优先使用内存的条件已经成熟；其次，使用大数据进行机器学习的需求越来越强烈，不再是早先年那种数据分析的简单计算需求。而机器学习的算法大多需要很多轮迭代，Spark 的 stage 划分相比 Map 和 Reduce 的简单划分，有更加友好的编程体验和更高效的执行效率。
### 工作原理
![Spark架构图](https://github.com/star2478/server-tech-tree/blob/master/img/spark-arch.png)

* 首先，Spark 应用程序启动在自己的 JVM 进程里，即 Driver 进程，启动后调用 SparkContext 初始化执行配置和输入数据。SparkContext 启动 DAGScheduler 构造执行的 DAG 图，切分成最小的执行单位也就是计算任务。
* 然后 Driver 向 Cluster Manager 请求计算资源，用于 DAG 的分布式计算。Cluster Manager 收到请求以后，将 Driver 的主机地址等信息通知给集群的所有计算节点 Worker。
* Worker 收到信息以后，根据 Driver 的主机地址，跟 Driver 通信并注册，然后根据自己的空闲资源向 Driver 通报自己可以领用的任务数。Driver 根据 DAG 图开始向注册的 Worker 分配任务。
* Worker 收到任务后，启动 Executor 进程开始执行任务。Executor 先检查自己是否有 Driver 的执行代码，如果没有，从 Driver 下载执行代码，通过 Java 反射加载后开始执行。



# 数据库
  * 真正的分布式数据库，与传统的mysql有何不同
  * nosql
    * 键值型：redis、levelDB
    * 文档型：MongoDB、couchDB
    * 搜索引擎：Elasticsearch、splunk、solr
    * 列式：HBase、Cassandra、ClickHouse（由俄罗斯Yandex 公司开源的面向 OLAP 的分布式列式数据库，能够使用 SQL 查询生成实时数据报告。有几倍于 GreenPlum 等引擎的性能优势。在大数据领域没有走 Hadoop 生态，而是采用 Local attached storage 作为存储，这样整个 IO 可能就没有 Hadoop 那一套的局限。）
    * 图形数据库：Neo4j、ArangoDB、Titan
  * tdingine：IoT大数据平台，super table创新模型
  * 国内数据库：华为GaussDB、阿里OceanBase
  * Mycat：数据库中间件，可通过它来组织数据库的分离读写和分库分表
  * MPP（大规模并行处理）数据库：开源中比较流行的有Greenplum、TiDB、Postgresql XC、HAWQ等，商用的如南大通用的GBase、睿帆科技的雪球DB、华为的LibrA等等，不同的MPP数据库的侧重点也不一样，如TiDB更侧重于分布式OLTP场景，Greenplum更侧重于分布式OLAP场景，这些MPP数据库基本都提供了类似Postgresql、Oracle、MySQL那样的SQL标准支持能力，能把一个查询解析为分布式的执行计划分发到每台机器上并行执行，最终由数据库本身汇总数据进行返回，也提供了诸如权限管理、分库分表、事务、数据副本等能力，并且大多能够支持100个节点以上的集群，大大降低了数据库运维的成本，并且使数据库也能够实现水平扩展。

# BI
  * cboard开源
  * tableau商业

# 一致性
![数据一致性方案](http://upload-images.jianshu.io/upload_images/2119886-ec896ce10581bc1a.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1080/q/50)
* 常用的一致性算法：raft、paxos、pow等
 * pow解决了共识问题（数据可信同步），还解决了双花问题？
 * 区块链SVP什么场景下用到?拜占庭将军问题如何解决，为何raft和paxio不能解决?

* 默克尔树
 * 数据完整性或一致性校验的方法
 * btc有一个，etc有三个，见极客
![默克尔树](https://github.com/star2478/server-tech-tree/blob/master/img/blockchain-merkletree.png)

* CAP
 * “在网络发生阻断（partition）时，你只能选择数据的一致性（consistency）或可用性（availability），无法两者兼得”。论点比较直观：如果网络因阻断而分隔为二，在其中一边我送出一笔交易：“将我的十元给 A”；在另一半我送出另一笔交易：“将我的十元给 B ”。此时系统要么是，a）无可用性，即这两笔交易至少会有一笔交易不会被接受；要么就是，b）无一致性，一半看到的是 A 多了十元而另一半则看到 B 多了十元。要注意的是，CAP 理论和扩展性（scalability）是无关的，在分片（sharded）或非分片的系统皆适用


# [淘宝数据魔方技术架构解析（2011）](https://link.juejin.im/?target=http%3A%2F%2Fhistory.programmer.com.cn%2F7578%2F)
  * 数据魔方已下线，转为市场行情专业版
![1.png](https://upload-images.jianshu.io/upload_images/2119886-dd4f7b69c459b428.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
  * 其业务属于OLAP，针对数据非实时写入场景，但读支持实时和非实时
  * 计算层：融合了批处理（云梯）和流计算（银河）。主要是离线批处理，但仍然有些数据要求时效性，例如针对搜索词的统计数据，引入了流计算，存入存储层NoSQL中
  * 存储层：融合关系型数据库（MyFOX）和NoSQL（Prom）。数据中间层glider相当于存储层对外的网关，对外屏蔽MyFOX和Prom的差异，提供统一的基于HTTP协议restful格式的数据查询接口
    * MyFOX是一个分布式MySQL集群的查询代理层，使得分区对客户端完全透明。进行了冷热数据分离，热数据使用15k转/m SAS接口磁盘（应该是SSD）存储，冷数据使用7.5k转/m SATA接口磁盘存储，前者成本是后者的三倍
    * Porm弥补多属性场景的查询，基于HBase。主要用在以各种商品属性作为筛选条件查询交易数据，所以用商品属性+属性值作为row-key，用交易列表和交易详情相关字段作为column-family
    * 存储层使用了多级缓存（分别位于glider、MyFOX、Prom不同层）以提高查询效率，并使用缓存所有数据（包括空值）减少缓存穿透

# 文件系统
磁盘冗余阵列rain，rain5使用奇偶校验（即异或运算）来恢复数据，失败节点太多就无法恢复了->跨机房备份，不同网络延迟多少->一个节点（物理机/虚机）装不下，采用分片分区，Redis、kafka、es的不同见[这里](https://mp.weixin.qq.com/s?__biz=MzAxOTc0NzExNg==&mid=2665516450&idx=1&sn=a6c7ec299a14a5fc85ea33ac9bc9cd6f&chksm=80d675e1b7a1fcf7f367a0186ec8caf2cac6ea0ef57ba9ed2c2d5a15ff227c7907b4a397f854&xtrack=1&scene=90&subscene=93&sessionid=1562289393&clicktime=1562289400&ascene=56&devicetype=android-27&version=2700043c&nettype=3gnet&abtest_cookie=BQABAAoACwASABMAFQAFACOXHgBWmR4AzpkeAPKZHgALmh4AAAA%3D&lang=zh_CN&pass_ticket=jj7cA9U%2BcUV5%2Bacw3nuvYMJ8smsVcqYHF%2BHqTfQuq%2FwNG%2BoSYibCq6cUqOcEpqpq&wx_header=1)

# 流批融合框架Lambda 架构
  * 目前影响最深刻的大数据处理架构，它的核心思想是将不可变的数据以追加的方式并行写到批和流处理系统内，随后将相同的计算逻辑分别在流和批系统中实现，并且在查询阶段合并流和批的计算视图并展示给用户
  * Lambda的提出者 Nathan Marz 还假定了批处理相对简单不易出现错误，而流处理相对不太可靠，因此流处理器可以使用近似算法，快速产生对视图的近似更新，而批处理系统会采用较慢的精确算法，产生相同视图的校正版本

# log
ELK。log的设计思路和目标？？

# 存储引擎的速度分级
* L1 cache reference：0.5ns
* Branch mispredict：5ns
* L2 cache reference：7ns
* Mutex lock/unlock：25ns
* Main memory reference：100ns
* Compress 1K bytes with Zippy：3 000ns
* Send 2K bytes over 1Gbps network：20 000ns
* Read 1MB sequentially from Memory：250 000ns
* Round trip within same datacenter：500 000ns(0.5ms)
* Disk seek：10 000 000ns(10ms)
* Read 1MB sequentially from disk：20 000 000ns(20ms)
* Send package CA->Netherlands->CA：150 000 000ns(150ms)
* 对于15000转/分钟的磁盘，寻道时间大概在2-8毫秒之间。而旋转延时平均在2毫米左右。


# Redis性能
数据包控制在1000字节以下，不开启pipeline吞吐量约为6w/s，使用pipeline约为10w/s。1000字节是redis性能的分水岭
