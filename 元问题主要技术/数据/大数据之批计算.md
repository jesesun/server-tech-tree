# 大数据之批计算
* 另一方面，你看一下 Hadoop 几个主要产品的架构设计，就会发现它们都有相似性，都是一主多从的架构方案。HDFS，一个 NameNode，多个 DataNode；MapReduce 1，一个 JobTracker，多个 TaskTracker；Yarn，一个 ResourceManager，多个 NodeManager。
* 事实上，很多大数据产品都是这样的架构方案：Storm，一个 Nimbus，多个 Supervisor；Spark，一个 Master，多个 Slave。
* 大数据因为要对数据和计算任务进行统一管理，所以和互联网在线应用不同，需要一个全局管理者。而在线应用因为每个用户请求都是独立的，而且为了高性能和便于集群伸缩，会尽量避免有全局管理者。

## MapReduce
![MapReduce](https://github.com/star2478/server-tech-tree/blob/master/img/MapReduce.png)
* 关键能力：代码到数据所在地执行(边缘计算)、map输出的同一个key结果会shuffle到同一个reduce进程节点处理(对key做hash%reduce数，就可保证到同一个ID的reduce上，但当同一个key数据大且reduce数不够时，可能会压垮reduce)

## Spark
![Spark](https://github.com/star2478/server-tech-tree/blob/master/img/spark.png)
* 注：上图，application master可能和MapReduce作业(图中容器)不在同一个datanode
* spark：RDD 弹性数据集（Resilient Distributed Datasets），Spark 分布式计算的数据分片、任务调度都是以 RDD 为单位展开的，每个 RDD 分片都会分配到一个执行进程去处理。
* RDD 上定义的函数分两种，一种是转换（transformation）函数，这种函数的返回值还是 RDD；另一种是执行（action）函数，这种函数不再返回 RDD。
* RDD 定义了很多转换操作函数(很像rxjava)，比如有计算map(func)、过滤filter(func)、合并数据集union(otherDataset)、根据 Key 聚合reduceByKey(func, [numPartitions])、连接数据集join(otherDataset, [numPartitions])、分组groupByKey([numPartitions]) 等十几个函数。
* 相对RM的map和reduce，spark一个作业可以支持更多函数的计算，速度更快。spark也有关键的shuffle过程(一个数据集中的多个数据分片需要进行分区传输，写入到另一个数据集的不同分片中)，shuffle会把上一次计算结果按key进行数据分片，不同分片传到不同的下一个计算节点
* spark会将一个作业所有函数转化成dag(有向无环图，见上图)，rdd是一个个数据集，如图上，一个rdd包含了多个小块，每个小块代表一个数据分片，相当于rdd是逻辑上的数据集，里面的一个分片对应一个物理位置
* spark会将一个作业所有函数转化成dag(有向无环图，见上图)，rdd是一个个数据集，如图上，一个rdd包含了多个小块，每个小块代表一个数据分片，相当于rdd是逻辑上的数据集，里面的一个分片对应一个物理位置。
* 有些函数不会shuffle，比如上图map，c和d两个rdd数据量都一样。有些需要shuffle，比如上图groupby，rdd a转成一个数据量完全不同的rdd b，a结果要shuffle出去形成rdd b
* Spark 有三个主要特性：RDD 的编程模型更简单，DAG 切分的多阶段计算过程更快速，使用内存存储中间计算结果更高效。这三个特性使得 Spark 相对 Hadoop MapReduce 可以有更快的执行速度，以及更简单的编程实现。
* 首先，Spark 在 2012 年左右开始流行，那时内存的容量提升和成本降低已经比 MapReduce 出现的十年前强了一个数量级，Spark 优先使用内存的条件已经成熟；其次，使用大数据进行机器学习的需求越来越强烈，不再是早先年那种数据分析的简单计算需求。而机器学习的算法大多需要很多轮迭代，Spark 的 stage 划分相比 Map 和 Reduce 的简单划分，有更加友好的编程体验和更高效的执行效率
### 工作原理
![Spark架构图](https://github.com/star2478/server-tech-tree/blob/master/img/spark-arch.png)

* 首先，Spark 应用程序启动在自己的 JVM 进程里，即 Driver 进程，启动后调用 SparkContext 初始化执行配置和输入数据。SparkContext 启动 DAGScheduler 构造执行的 DAG 图，切分成最小的执行单位也就是计算任务。
* 然后 Driver 向 Cluster Manager 请求计算资源，用于 DAG 的分布式计算。Cluster Manager 收到请求以后，将 Driver 的主机地址等信息通知给集群的所有计算节点 Worker。
* Worker 收到信息以后，根据 Driver 的主机地址，跟 Driver 通信并注册，然后根据自己的空闲资源向 Driver 通报自己可以领用的任务数。Driver 根据 DAG 图开始向注册的 Worker 分配任务。
* Worker 收到任务后，启动 Executor 进程开始执行任务。Executor 先检查自己是否有 Driver 的执行代码，如果没有，从 Driver 下载执行代码，通过 Java 反射加载后开始执行。