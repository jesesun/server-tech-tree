```
1、数据存储(解决能够读写数据问题，各种rmdb、nosql、文件系统)
2、数据处理效率(解决能够更快读写数据问题——大数据方面：计算框架来提速 spark内存来提速 流计算来提速，普通量级数据：SQL简化读写，采集，过滤)
3、数据存储空间(解决用更少空间存数据问题，是否可以纳入到数据存储中?)
```

# 数据格式
  * 格式化
  * 半格式化
  * 文本
  * 富文本
  * 数据的扩展性：SQL-数据处理、json-数据存储、分库（拆不同）分表（拆相近）-数据存储

# 数据技术关系
![数据技术关系图](https://github.com/star2478/server-tech-tree/blob/master/img/data-tech-relationship.png)


# 大数据相关技术
![大数据工具](https://github.com/star2478/server-tech-tree/blob/master/img/bigdata-tool.png)
```
大数据组件生态圈
· HDFS =====> 解决存储问题
· MapReduce =====> 批处理框架
· Spark =====> 混合框架，能处理批处理和流计算
· Flink =====> 混合框架，能处理批处理和流计算
· Storm =====> 流计算框架
· Yarn =====> 资源协调者
· Zookeeper =====> 分布式应用程序协调服务
· Flume =====> 日志收集系统
· Hive =====> 基于Hadoop的数仓工具
· HBase =====> 分布式、面向列的开源数据库
· Sqoop =====> 数据传递工具
· Scala =====> 多范式编程语言、面向对象和函数式编程的特性
· Elasticsearch =====> 大数据分布式弹性搜索引擎
```
* 在由很多台服务器组成的服务器集群中，某台服务器可能运行着 HDFS 的 DataNode 进程，负责 HDFS 的数据存储；同时也运行着 Yarn 的 NodeManager，负责计算资源的调度管理；而 MapReduce、Spark、Storm、Flink 这些批处理或者流处理大数据计算引擎则通过 Yarn 的调度，运行在 NodeManager 的容器（container）里面。至于 Hive、Spark SQL 这些运行在 MapReduce 或者 Spark 基础上的大数据仓库引擎，在经过自身的执行引擎将 SQL 语句解析成 MapReduce 或者 Spark 的执行计划以后，一样提交给 Yarn 去调度执行
* 大数据技术方向（待改正）：数据收集处理、推荐、实时计算、离线计算（基于数仓）、数据分析挖掘

# lambda架构
* 大数据平台常规架构
* 各大公司使用各种大数据工具搭建了lambda架构的大数据平台后，还要考虑实现自己的作业调度系统，实现所有大数据任务的排队、执行时间管理、全局任务监控等

# 批处理框架
* 另一方面，你看一下 Hadoop 几个主要产品的架构设计，就会发现它们都有相似性，都是一主多从的架构方案。HDFS，一个 NameNode，多个 DataNode；MapReduce 1，一个 JobTracker，多个 TaskTracker；Yarn，一个 ResourceManager，多个 NodeManager。
* 事实上，很多大数据产品都是这样的架构方案：Storm，一个 Nimbus，多个 Supervisor；Spark，一个 Master，多个 Slave。
* 大数据因为要对数据和计算任务进行统一管理，所以和互联网在线应用不同，需要一个全局管理者。而在线应用因为每个用户请求都是独立的，而且为了高性能和便于集群伸缩，会尽量避免有全局管理者。

## MapReduce
![MapReduce](https://github.com/star2478/server-tech-tree/blob/master/img/MapReduce.png)
* 关键能力：代码到数据所在地执行(边缘计算)、map输出的同一个key结果会shuffle到同一个reduce进程节点处理(对key做hash%reduce数，就可保证到同一个ID的reduce上，但当同一个key数据大且reduce数不够时，可能会压垮reduce)

## Spark
![Spark](https://github.com/star2478/server-tech-tree/blob/master/img/spark.png)
* 注：上图，application master可能和MapReduce作业(图中容器)不在同一个datanode
* spark：RDD 弹性数据集（Resilient Distributed Datasets），Spark 分布式计算的数据分片、任务调度都是以 RDD 为单位展开的，每个 RDD 分片都会分配到一个执行进程去处理。
* RDD 上定义的函数分两种，一种是转换（transformation）函数，这种函数的返回值还是 RDD；另一种是执行（action）函数，这种函数不再返回 RDD。
* RDD 定义了很多转换操作函数(很像rxjava)，比如有计算map(func)、过滤filter(func)、合并数据集union(otherDataset)、根据 Key 聚合reduceByKey(func, [numPartitions])、连接数据集join(otherDataset, [numPartitions])、分组groupByKey([numPartitions]) 等十几个函数。
* 相对RM的map和reduce，spark一个作业可以支持更多函数的计算，速度更快。spark也有关键的shuffle过程(一个数据集中的多个数据分片需要进行分区传输，写入到另一个数据集的不同分片中)，shuffle会把上一次计算结果按key进行数据分片，不同分片传到不同的下一个计算节点
* spark会将一个作业所有函数转化成dag(有向无环图，见上图)，rdd是一个个数据集，如图上，一个rdd包含了多个小块，每个小块代表一个数据分片，相当于rdd是逻辑上的数据集，里面的一个分片对应一个物理位置
* spark会将一个作业所有函数转化成dag(有向无环图，见上图)，rdd是一个个数据集，如图上，一个rdd包含了多个小块，每个小块代表一个数据分片，相当于rdd是逻辑上的数据集，里面的一个分片对应一个物理位置。
* 有些函数不会shuffle，比如上图map，c和d两个rdd数据量都一样。有些需要shuffle，比如上图groupby，rdd a转成一个数据量完全不同的rdd b，a结果要shuffle出去形成rdd b
* Spark 有三个主要特性：RDD 的编程模型更简单，DAG 切分的多阶段计算过程更快速，使用内存存储中间计算结果更高效。这三个特性使得 Spark 相对 Hadoop MapReduce 可以有更快的执行速度，以及更简单的编程实现。
* 首先，Spark 在 2012 年左右开始流行，那时内存的容量提升和成本降低已经比 MapReduce 出现的十年前强了一个数量级，Spark 优先使用内存的条件已经成熟；其次，使用大数据进行机器学习的需求越来越强烈，不再是早先年那种数据分析的简单计算需求。而机器学习的算法大多需要很多轮迭代，Spark 的 stage 划分相比 Map 和 Reduce 的简单划分，有更加友好的编程体验和更高效的执行效率
### 工作原理
![Spark架构图](https://github.com/star2478/server-tech-tree/blob/master/img/spark-arch.png)

* 首先，Spark 应用程序启动在自己的 JVM 进程里，即 Driver 进程，启动后调用 SparkContext 初始化执行配置和输入数据。SparkContext 启动 DAGScheduler 构造执行的 DAG 图，切分成最小的执行单位也就是计算任务。
* 然后 Driver 向 Cluster Manager 请求计算资源，用于 DAG 的分布式计算。Cluster Manager 收到请求以后，将 Driver 的主机地址等信息通知给集群的所有计算节点 Worker。
* Worker 收到信息以后，根据 Driver 的主机地址，跟 Driver 通信并注册，然后根据自己的空闲资源向 Driver 通报自己可以领用的任务数。Driver 根据 DAG 图开始向注册的 Worker 分配任务。
* Worker 收到任务后，启动 Executor 进程开始执行任务。Executor 先检查自己是否有 Driver 的执行代码，如果没有，从 Driver 下载执行代码，通过 Java 反射加载后开始执行。

# 流计算框架
## Storm
* nimbus 是集群的 Master，负责集群管理、任务分配等。supervisor 是 Slave，是真正完成计算的地方，每个 supervisor 启动多个 worker 进程，每个 worker 上运行多个 task，而 task 就是 spout 或者 bolt。storm将spout和bolt组成一个topology进行任务分发和关联。supervisor 和 nimbus 通过 ZooKeeper 完成任务分配、心跳检测等操作。
* Hadoop、Storm 的设计理念，其实是一样的，就是把和具体业务逻辑无关的东西抽离出来，形成一个框架，比如大数据的分片处理、数据的流转、任务的部署与执行等，开发者只需要按照框架的约束，开发业务逻辑代码(spout和bolt)，提交给框架执行就可以了。

# 混合计算框架
## flink
![Flink架构图](https://github.com/star2478/server-tech-tree/blob/master/img/flink.png)
* 不管流处理和批处理，对于flink来说就是数据源不同，用的是同一套执行引擎
* 支持对数据进行各种transformation(类似spark)
* 将数据按一个个时间window来切分处理
* Flink 的架构和 Hadoop 1 或者 Yarn 看起来也很像，JobManager 是 Flink 集群的管理者，Flink 程序提交给 JobManager 后，JobManager 检查集群中所有 TaskManager 的资源利用状况，如果有空闲 TaskSlot（任务槽），就将计算任务分配给它执行

# 计算资源管理框架
## yarn(对标mesos和docker)
* 独立部署一个resource manager节点集群，代码提交给它，它通过与nodemanager通讯，将某个MapReduce作业对应的application master程序分派到一个datanode的一个容器中执行，application master向resource manager汇报每个datanode的资源使用情况，resource manager通过fair或capacity算法分配map或reduce任务给不同datanode。resource manager；
* 每个datanode进程节点上部署一个nodemanager进程，将节点分成一个个容器，每个容器都拥有自己的cpu和内存等资源。
## mesos

> 注1：ApplicationMaster 向资源管理器申请计算资源时可以指定目标节点（数据分片所在节点），而如果系统资源能够满足，就会把mapreduce计算任务分发到指定的服务器上。如果资源不允许，比如目标节点非常繁忙，这时部分mapreduce计算任务可能会分配另外的服务器（数据分片不在本地，即做不到边缘计算，数据需要传到map和reduce程序所在节点）

> 注2：为何专门起一个application master来给resource manager汇报和为mr作业申请资源，而非利用node manager(这样一个datanode很可能就有datanode、nodemanager、application master三个进程了)?这是因为一是节点管理(nodemanager)和作业监控/资源申请(application master)进行解耦，可以兼容各种计算框架(MapReduce、spark等)，计算框架只要遵循yarn规范写applicationmaster即可实现自己的资源调度策略，二是每个MapReduce作业都对应一个application master，即每个作业可以定制化自己的资源调度策略，也有默认策略(fair或capacity)。可以说，application master是面向某个map和reduce应用的，而node manager是面向某个datanode的

# 数据采集和传输
* 爬虫
  * scrapy
* 在线系统数据如何导给大数据系统
  * sqoop：数据库与hdfs的批量数据同步(导入导出)
  * canal：数据库与hdfs的实时数据同步(导入导出)
  * flume：日志导入导出hdfs，flume还可以实现级联，即一个flume输出可以作为另一个输入
  * kafka：埋点或爬虫采集的数据经过格式化转换后通过 Kafka 等消息队列导入给hdfs

# 数据库
## RDBMS
  * 真正的分布式数据库，与传统的mysql有何不同
  * nosql
    * 键值型：redis、levelDB
    * 文档型：MongoDB、couchDB
    * 搜索引擎：Elasticsearch、splunk、solr
    * 列式：HBase、Cassandra、ClickHouse（由俄罗斯Yandex 公司开源的面向 OLAP 的分布式列式数据库，能够使用 SQL 查询生成实时数据报告。有几倍于 GreenPlum 等引擎的性能优势。在大数据领域没有走 Hadoop 生态，而是采用 Local attached storage 作为存储，这样整个 IO 可能就没有 Hadoop 那一套的局限。）
    * 图形数据库：Neo4j、ArangoDB、Titan
  * tdingine：IoT大数据平台，super table创新模型
  * 国内数据库：华为GaussDB、阿里OceanBase、阿里cobar->tddl->drds
  * Mycat：数据库中间件，可通过它来组织数据库的分离读写和分库分表
  * MPP（大规模并行处理）数据库：开源中比较流行的有Greenplum、TiDB、Postgresql XC、HAWQ等，商用的如南大通用的GBase、睿帆科技的雪球DB、华为的LibrA等等，不同的MPP数据库的侧重点也不一样，如TiDB更侧重于分布式OLTP场景，Greenplum更侧重于分布式OLAP场景，这些MPP数据库基本都提供了类似Postgresql、Oracle、MySQL那样的SQL标准支持能力，能把一个查询解析为分布式的执行计划分发到每台机器上并行执行，最终由数据库本身汇总数据进行返回，也提供了诸如权限管理、分库分表、事务、数据副本等能力，并且大多能够支持100个节点以上的集群，大大降低了数据库运维的成本，并且使数据库也能够实现水平扩展。
## NoSQL
* [看图轻松理解数据结构与算法系列(NoSQL存储-LSM树): https://mv.mbd.baidu.com/sm7cwm7?f=wf&u=c5e4660b3a7db8a8]
 * lsm树也是大部分nosql的核心结构，可以避免btree在rmdb上的不足，因为btree无法避免逻辑上相近的数据在物理上很远，导致经常放在一起查的数据比较耗时
* 列式存储vs行式存储
* 国内：阿里Doris、小米Pegasus、头条abase(貌似未开源)
  * Doris不关注底层存储(基于Berkeley DB)，主要关注分布式方面的改进，关键技术点包括：分区路由(数据一致性)、失效转移(可用性)、集群伸缩(伸缩性)
## NewSQL
## 链式数据库
* 贴吧
* 头条的拉链存储ByteGraph，计划以后开源。

# 数据仓库
* hive

### HBase
![HBase](https://github.com/star2478/server-tech-tree/blob/master/img/hbase.png)
* hbase仍然是主从模式，master记录元信息并注册到zk，client从zk获取数据key所在hregion然后去访问。hbase是根据数据的key来实现数据分片的，一个hregion就是一个分片，存储了某个范围key的数据
* 一个hfile是一个hdfs文件，自动做好分片备份，无需hbase自己做
* HBase其它特性
 * HBASE伸缩性：扩容容易，缩容难?hbase单个hregion存入数据量太大会自动拆成多个并自动迁到其他节点，如果数据变少了呢，hbase有考虑数据缩容情况吗，如果节点数缩容是不是也意味着数据要做缩容？实际上一般数据也不会变少，所以一般不会考虑数据的缩容，一般只考虑应用的缩容
 * HBASE扩展性：nosql都一样，可以扩展无数个字段
 * HBASE性能：通过 LSM 树，使数据可以通过连续写磁盘的方式保存数据，提高数据写入性能。lsm树原理见上面链接，lsm不是一种具体数据结构，而是可以使用不同数据结构组成的设计思想，比如c0 tree可以使用avl树来实现，其他级别tree使用b 树来实现，c0 tree在内存，其他级别tree在磁盘。先写log(顺序写磁盘很快)再写c0 tree，这样一来断电后虽然c0 tree会消失，但可以根据log恢复。

# BI
  * cboard开源
  * tableau商业

# 一致性
## CAP
* CAP 定理关注的是对数据的读写操作，而不是分布式系统的所有功能，它要求分布式系统节点间是互相连接且有数据共享的，例如 Memcache 的集群中节点相互间没有连接和数据共享，因此不是 CAP 定理讨论的对象，同理 ZooKeeper 的选举机制也不是 CAP 探讨的对象。
* cap：一致性属于数据；可用性；网络分区容错属于?
## BASE
## 锁
* 追求性能->多线程并发->数据一致性问题->锁和分布式锁

## 一致性算法
![数据一致性方案](http://upload-images.jianshu.io/upload_images/2119886-ec896ce10581bc1a.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1080/q/50)
* 常用的一致性算法：raft、paxos、pow等
 * pow解决了共识问题（数据可信同步），还解决了双花问题？
 * 区块链SVP什么场景下用到?拜占庭将军问题如何解决，为何raft和paxio不能解决?
 * ZooKeeper 通过 zab(Paxos简化版) 选举算法实现数据强一致性(zk是多主，即所有节点都存全量数据)，这样一来zk本身就避免了脑裂，就可以充当其他系统主从的裁判了 (即zk保存谁是主谁是从，主失去心跳则从可以换成主)，如果zk这个角色无法避免脑裂，整个系统(无论zk还是主从系统)都会有脑裂问题，元数据不一致，元数据出错会引发各种严重问题，比如下游数据不一致、数据错误、应用执行失败
* 脑裂问题

* 默克尔树
 * 数据完整性或一致性校验的方法
 * btc有一个，etc有三个，见极客
![默克尔树](https://github.com/star2478/server-tech-tree/blob/master/img/blockchain-merkletree.png)

* CAP
 * “在网络发生阻断（partition）时，你只能选择数据的一致性（consistency）或可用性（availability），无法两者兼得”。论点比较直观：如果网络因阻断而分隔为二，在其中一边我送出一笔交易：“将我的十元给 A”；在另一半我送出另一笔交易：“将我的十元给 B ”。此时系统要么是，a）无可用性，即这两笔交易至少会有一笔交易不会被接受；要么就是，b）无一致性，一半看到的是 A 多了十元而另一半则看到 B 多了十元。要注意的是，CAP 理论和扩展性（scalability）是无关的，在分片（sharded）或非分片的系统皆适用


# [淘宝数据魔方技术架构解析（2011）](https://link.juejin.im/?target=http%3A%2F%2Fhistory.programmer.com.cn%2F7578%2F)
  * 数据魔方已下线，转为市场行情专业版
![1.png](https://upload-images.jianshu.io/upload_images/2119886-dd4f7b69c459b428.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
  * 其业务属于OLAP，针对数据非实时写入场景，但读支持实时和非实时
  * 计算层：融合了批处理（云梯）和流计算（银河）。主要是离线批处理，但仍然有些数据要求时效性，例如针对搜索词的统计数据，引入了流计算，存入存储层NoSQL中
  * 存储层：融合关系型数据库（MyFOX）和NoSQL（Prom）。数据中间层glider相当于存储层对外的网关，对外屏蔽MyFOX和Prom的差异，提供统一的基于HTTP协议restful格式的数据查询接口
    * MyFOX是一个分布式MySQL集群的查询代理层，使得分区对客户端完全透明。进行了冷热数据分离，热数据使用15k转/m SAS接口磁盘（应该是SSD）存储，冷数据使用7.5k转/m SATA接口磁盘存储，前者成本是后者的三倍
    * Porm弥补多属性场景的查询，基于HBase。主要用在以各种商品属性作为筛选条件查询交易数据，所以用商品属性+属性值作为row-key，用交易列表和交易详情相关字段作为column-family
    * 存储层使用了多级缓存（分别位于glider、MyFOX、Prom不同层）以提高查询效率，并使用缓存所有数据（包括空值）减少缓存穿透

# 文件系统
* hdfs
  * namenode和datanode
  * hdfs的可用性
* 磁盘冗余阵列rain，rain5使用奇偶校验（即异或运算）来恢复数据，失败节点太多就无法恢复了->跨机房备份，不同网络延迟多少->一个节点（物理机/虚机）装不下，采用分片分区，Redis、kafka、es的不同见[这里](https://mp.weixin.qq.com/s?__biz=MzAxOTc0NzExNg==&mid=2665516450&idx=1&sn=a6c7ec299a14a5fc85ea33ac9bc9cd6f&chksm=80d675e1b7a1fcf7f367a0186ec8caf2cac6ea0ef57ba9ed2c2d5a15ff227c7907b4a397f854&xtrack=1&scene=90&subscene=93&sessionid=1562289393&clicktime=1562289400&ascene=56&devicetype=android-27&version=2700043c&nettype=3gnet&abtest_cookie=BQABAAoACwASABMAFQAFACOXHgBWmR4AzpkeAPKZHgALmh4AAAA%3D&lang=zh_CN&pass_ticket=jj7cA9U%2BcUV5%2Bacw3nuvYMJ8smsVcqYHF%2BHqTfQuq%2FwNG%2BoSYibCq6cUqOcEpqpq&wx_header=1)

# 对象存储
# 块存储

# 流批融合框架Lambda 架构
  * 目前影响最深刻的大数据处理架构，它的核心思想是将不可变的数据以追加的方式并行写到批和流处理系统内，随后将相同的计算逻辑分别在流和批系统中实现，并且在查询阶段合并流和批的计算视图并展示给用户
  * Lambda的提出者 Nathan Marz 还假定了批处理相对简单不易出现错误，而流处理相对不太可靠，因此流处理器可以使用近似算法，快速产生对视图的近似更新，而批处理系统会采用较慢的精确算法，产生相同视图的校正版本

# log
ELK。log的设计思路和目标？？

# 网络
SDN、FLAT

# 配置中心
* 代表：Archaius、Spring Cloud Config
* 关键技术
